# -*- coding: utf-8 -*-
"""EMAIL_SPAM_CLASIFIER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hax5rUrbsWCZJw9Z951c5NZzoYP3fVBa
"""

import numpy as np
import pandas as pd

df=pd.read_csv('/content/Untitled spreadsheet - spam.csv')
df.head()

df.shape

#data cleaninhg
#EDA
#TEXT PREPROCESSING
#MODEL BUILDING 
#EVALUATION
#IMPROVEEMNT

#DATA CLEANING

df.info()

df.columns

df=df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1)

df.columns=['x', 'y']

df.head(2)

from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()

df['x']=encoder.fit_transform(df['x'])



df.head()

#checking missing values
df.isnull().sum()

#check for duplicated values 
df.duplicated().sum()

df=df.drop_duplicates(keep='first')

df.duplicated().sum()

df.shape

#2: EDA

df['x'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['x'].value_counts(),labels=['ham','spam'],autopct='%0.2f')
plt.show()

#data is not balanced

# one morew analysis we are doing
# to see  how many characters ,words,spaces are used in the sms

import nltk
nltk.download('punkt')

pip install nltk

df['num_characters']=df['y'].apply(len)

df.head()

#no of words

df['y'].apply(lambda x:nltk.word_tokenize(x))

df['num_words']=df['y'].apply(lambda x:len(nltk.word_tokenize(x)))

#no of sentrences
df['y'].apply(lambda x:nltk.sent_tokenize(x))

df['num_sentences']=df['y'].apply(lambda x:len(nltk.word_tokenize(x)))

df.head()

df[['num_characters','num_words','num_sentences']].describe()

#ham mesages 
df[df['x']==0][['num_characters','num_words','num_sentences']].describe()

#spam mesages 
df[df['x']==1][['num_characters','num_words','num_sentences']].describe()

import seaborn as sns

sns.histplot(df[df['x']==0]['num_characters'])
sns.histplot(df[df['x']==1]['num_characters'],color='red')

sns.pairplot(df,hue='x')

df.corr()

sns.heatmap(df.corr(),annot=True)

#3: data preprocesisng 
#lowr case,tokenisation,remoing special characters,
#removing words and punctuation, stemming(dance, dancing into one because all are same )

import string
string.punctuation

def transfrom_text(text):
  text=text.lower()
  text=nltk.word_tokenize(text)
  return text

from wordcloud import WordCloud
wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')

plt.figure(figsize=(15,15))
spam_wc=wc.generate(df[df['x']==1]['y'].str.cat(sep=''))
plt.imshow(spam_wc)

plt.figure(figsize=(15,15))
ham_wc=wc.generate(df[df['x']==0]['y'].str.cat(sep=''))
plt.imshow(ham_wc)

df[df['x']==1]

spam_corpus=[]
for msg in df[df['x']==1]['y'].tolist():
  for word in msg.split():
      spam_corpus.append(word)

len(spam_corpus)

from collections import Counter 
Counter(spam_corpus)

Counter(spam_corpus).most_common(30)

pd.DataFrame(Counter(spam_corpus).most_common(30))

sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.show()

ham_corpus=[]
for msg in df[df['x']==1]['y'].tolist():
  for word in msg.split():
      ham_corpus.append(word)

len(spam_corpus)

from collections import Counter 
pd.DataFrame(Counter(ham_corpus).most_common(30))

sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.show()

# model building a sit works best on textual data

#from sklearn.feature_extraction.text import CountVectorizer,#this howe less accuracy
from sklearn.feature_extraction.text import TfidfVectorizer
Tfidf=TfidfVectorizer()

m=Tfidf.fit_transform(df['y']).toarray()

"""we can improve the models by using this also """

#WE CAN USE OF NEW COLUMNS CREATD BY US 
#m=np.hstac((DF['NUM_CHARACTERS'].VALUES_RESHAPE(-1,1))) , write all in lower csase





m.shape

#from sklearn.preprocessing import MinMaxScaler      , we cam use this for hypertuning , we didnt went with the standard scaler as it will negative values which is not accpeted by NB
#scaler=MinMaxScaler()
#m=scaler.fit_transform()

n=df['x'].values
n

from sklearn.model_selection import train_test_split
m_train,m_test,n_train,n_test=train_test_split(m,n, test_size=.2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score
gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()

gnb.fit(m_train,n_train)
n_pred=gnb.predict(m_test)
print(accuracy_score(n_test,n_pred))
print(confusion_matrix(n_test,n_pred))
print(precision_score(n_test,n_pred))

mnb.fit(m_train,n_train)
n_pred=mnb.predict(m_test)     # we will go with this as the precison score is too good usingTfidf
print(accuracy_score(n_test,n_pred))
print(confusion_matrix(n_test,n_pred))
print(precision_score(n_test,n_pred))

bnb.fit(m_train,n_train)
n_pred=bnb.predict(m_test)
print(accuracy_score(n_test,n_pred))
print(confusion_matrix(n_test,n_pred))
print(precision_score(n_test,n_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB,GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier()
mnb = MultinomialNB()
gnb = GaussianNB()
dtc = DecisionTreeClassifier(max_depth=5)
lrc = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
bc = BaggingClassifier(n_estimators=60, random_state=2)
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=70,random_state=2)
xgb = XGBClassifier(n_estimators=45,random_state=2)

clfs = {
        'SVC' : svc,
        'KN' : knc, 
        'NB': mnb, 
        'DT': dtc, 
        'LR': lrc, 
        'RF': rfc, 
        'AdaBoost': abc,
        'BgC': bc, 
        'ETC': etc,
        'GBDT':gbdt,
        'xgb':xgb
    }

def train_classifier(clf,m_train,n_train,m_test,n_test):
    clf.fit(m_train,n_train)
    n_pred = clf.predict(m_test)
    accuracy = accuracy_score(n_test,n_pred)
    precision = precision_score(n_test,n_pred)
    return accuracy,precision

train_classifier(svc,m_train,n_train,m_test,n_test)

train_classifier(knc,m_train,n_train,m_test,n_test)

train_classifier(gnb,m_train,n_train,m_test,n_test)

accuracy_scores = []
precision_scores = []
for name,clf in clfs.items():
  current_accuracy,current_precision = train_classifier(clf, m_train,n_train,m_test,n_test)
  print("For ",name)
  print("Accuracy - ",current_accuracy)
  print("Precision - ",current_precision)
  accuracy_scores.append(current_accuracy)
  precision_scores.append(current_precision)

performance_df=pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)

performance_df

#model_improvement
#using max_features in Tfivdf do the hypertuning 
# we can sacle the values also